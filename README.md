# Real-time WebRTC Object Detection Demo

A real-time multi-object detection system that streams video from a phone via WebRTC, performs inference (server-side or WASM), and displays detection overlays with sub-second latency.

## Quick Start (One Command)

```bash
# Clone and start (WASM mode - works on any laptop)
git clone <repo-url>
cd <repo-name>
./start.sh

# Or with Docker
docker-compose up --build

# For external access (phone on different network)
./start.sh --ngrok
```

## ğŸ“± Phone Connection

1. Start the system: `./start.sh`
2. Open http://localhost:3000 on your laptop
3. **Scan QR code** with your phone OR visit the displayed URL
4. Allow camera access when prompted
5. Point camera at objects - see real-time detection overlays on laptop

## ğŸ”§ Mode Switching

### WASM Mode (Default - Low Resource)
```bash
MODE=wasm ./start.sh
# or
docker-compose up --build
```
- âœ… Runs on modest laptops (no GPU needed)
- âœ… ~320Ã—240 input, 10-15 FPS processing
- âœ… Client-side inference (ONNX Runtime WASM)
- âœ… Automatic model download and setup

### Server Mode (High Performance)  
```bash
MODE=server ./start.sh
# or
MODE=server docker-compose up --build
```
- ğŸš€ Higher resolution processing
- ğŸš€ Faster inference with server-side GPU/CPU
- ğŸš€ Python ONNX Runtime backend
- âš¡ Requires Python dependencies (see server/requirements.txt)

## ğŸ“Š Benchmarking

Generate metrics.json with 30-second benchmark:

```bash
# After system is running
./bench/run_bench.sh --duration 30 --mode wasm

# Check results
cat metrics.json
```

**Example Output:**
```json
{
  "median_latency": 65,
  "p95_latency": 120,
  "processed_fps": 12.5,
  "uplink_kbps": 500,
  "downlink_kbps": 200
}
```

## ğŸ”— External Access (Phone on Different Network)

If your phone can't reach your laptop directly:

```bash
# Install ngrok first: npm install -g ngrok
./start.sh --ngrok
```

This exposes your localhost via ngrok tunnel - use the displayed https:// URL on your phone.

## ğŸ“ Project Structure

```
â”œâ”€â”€ components/           # React UI components
â”‚   â”œâ”€â”€ video-stream.tsx     # Main video display + overlays  
â”‚   â”œâ”€â”€ phone-camera.tsx     # Phone camera interface
â”‚   â”œâ”€â”€ detection-overlay.tsx # Bounding box renderer
â”‚   â””â”€â”€ metrics-display.tsx  # Performance metrics
â”œâ”€â”€ lib/                  # Core WebRTC and inference logic
â”‚   â”œâ”€â”€ webrtc-manager.ts    # Browser-side WebRTC + WASM
â”‚   â”œâ”€â”€ webrtc-client.ts     # Phone-side WebRTC client
â”‚   â””â”€â”€ types.ts             # TypeScript definitions
â”œâ”€â”€ server/               # Python inference server  
â”‚   â”œâ”€â”€ main.py              # WebRTC server + ONNX inference
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ bench/                # Benchmarking tools
â”‚   â””â”€â”€ run_bench.sh         # Metrics collection script
â”œâ”€â”€ models/               # ONNX model files (place here)
â”‚   â”œâ”€â”€ yolov5n.onnx         # Lightweight YOLO model
â”‚   â””â”€â”€ mobile-ssd-v1.onnx   # Alternative SSD model
â”œâ”€â”€ docker-compose.yml    # Container orchestration
â”œâ”€â”€ Dockerfile           # Multi-stage container build
â””â”€â”€ start.sh             # Main startup script
```

## ğŸ› ï¸ Troubleshooting

### Phone Won't Connect
- **Same WiFi**: Ensure phone and laptop on same network
- **Firewall**: Check if port 3000 is blocked
- **Use ngrok**: `./start.sh --ngrok` for external access

### No Detection Overlays
- Check browser console for errors
- Verify model files in `models/` directory  
- Try switching modes: `MODE=wasm ./start.sh`

### High CPU Usage
- Reduce resolution: Edit `wasmInputSize` in lib/webrtc-manager.ts
- Lower FPS: Increase timeout in `processFrame()` 
- Switch to server mode: `MODE=server ./start.sh`

### Alignment Issues
- Ensure `capture_ts` timestamps match between phone and browser
- Check detection overlay canvas sizing
- Verify normalized coordinates [0,1] in detection output

## ğŸ“ˆ Performance Notes

**WASM Mode (Default)**:
- Input: 320Ã—240 â†’ ~2 FPS processing  
- Median latency: ~65ms
- CPU usage: ~30% on Intel i5
- Memory: ~200MB

**Server Mode**:
- Input: 640Ã—480 â†’ ~15 FPS processing
- Median latency: ~45ms  
- CPU usage: ~50% on Intel i5
- Memory: ~500MB

## ğŸš€ Next Improvements

1. **Model quantization**: INT8 models for 2x speed boost
2. **Adaptive quality**: Dynamic resolution based on network/CPU
3. **Multi-phone support**: Handle multiple concurrent streams
4. **Edge deployment**: ARM64 containers for edge inference  
5. **WebCodecs**: Hardware video encoding for lower latency

## ğŸ§° Dependencies

**Runtime**:
- Node.js 18+
- Python 3.9+ (server mode)
- Modern browser with WebRTC support

**Models**: 
- Place ONNX models in `models/` directory
- Supports YOLOv5, MobileNet-SSD, custom COCO models
- Auto-downloads if missing

**Optional**:
- Docker & Docker Compose
- ngrok (for external access)
- bc (for benchmark calculations)

---

Built with Next.js, WebRTC, ONNX Runtime, and Socket.IO